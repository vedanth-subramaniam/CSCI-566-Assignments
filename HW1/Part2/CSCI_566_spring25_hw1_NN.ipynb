{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKuuwdlko03B"
      },
      "source": [
        "# **CSCI 566 - Spring 2025 - Homework 1**\n",
        "# **Problem 2: Basics of Neural Networks**\n",
        "\n",
        "## **Learning Objective**\n",
        "In this problem, you will implement a **multi-layer fully connected neural network** from scratch. The goal is to:\n",
        "- Understand and implement **forward and backward passes** of neural network layers.\n",
        "- Develop **different optimization techniques** to train the network.\n",
        "- Train and evaluate a model from scratch.\n",
        "\n",
        "---\n",
        "\n",
        "## **Provided Code**\n",
        "We have provided the skeleton of classes that you need to complete:\n",
        "- **Forward checking** and **gradient checking** functions are included to verify your implementations.\n",
        "- Helper functions for dataset loading and visualization.\n",
        "\n",
        "---\n",
        "\n",
        "## **TODOs**\n",
        "You need to implement the following:\n",
        "\n",
        "1. **Forward and backward passes** for standard neural network layers.\n",
        "2. Implement the following **loss functions**:\n",
        "   - Mean Squared Error (MSE)\n",
        "   - Softmax with Cross-Entropy\n",
        "3. Implement **common optimization algorithms**:\n",
        "   - Stochastic Gradient Descent (SGD)\n",
        "   - Adam\n",
        "   - RMSprop\n",
        "4. **Training a neural network** from scratch and tuning hyperparameters.\n",
        "5. **Inline questions**: You are required to answer conceptual questions within the notebook.\n",
        "\n",
        "---\n",
        "\n",
        "## **Dataset: Fashion MNIST**\n",
        "Fashion MNIST consists of **70,000 grayscale images** of size **28Ã—28** in **10 classes**, including:\n",
        "- T-shirts\n",
        "- Sneakers\n",
        "- Bags\n",
        "- Dresses, etc.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "44IJP67WrC6z"
      },
      "outputs": [],
      "source": [
        "!pip install matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u03rNM8xpHxb"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "%matplotlib inline\n",
        "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
        "plt.rcParams['image.interpolation'] = 'nearest'\n",
        "plt.rcParams['image.cmap'] = 'gray'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TJc3_37g0jLg"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from torchvision.datasets import FashionMNIST\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def _extract_arrays(dset):\n",
        "    x = dset.data.numpy().astype(np.float32) / 255.0\n",
        "    y = np.array(dset.targets, dtype=np.int64)\n",
        "    return x, y\n",
        "\n",
        "def fashionMnist():\n",
        "    download = not os.path.isdir(\"fashionMnist-batches-py\")\n",
        "    dset_train = FashionMNIST(root=\".\", download=download, train=True)\n",
        "    dset_test = FashionMNIST(root=\".\", train=False)\n",
        "    x_train, y_train = _extract_arrays(dset_train)\n",
        "    x_test_val, y_test_val = _extract_arrays(dset_test)\n",
        "    x_test, x_val, y_test, y_val = train_test_split(\n",
        "        x_test_val, y_test_val, test_size=0.2, random_state=42\n",
        "    )\n",
        "    return x_train, y_train, x_test, y_test, x_val, y_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MM5nqQg5582e"
      },
      "outputs": [],
      "source": [
        "X_train, y_train, X_test, y_test, X_val, y_val = fashionMnist()\n",
        "\n",
        "data = {\n",
        "    \"data_train\": X_train,\n",
        "    \"labels_train\": y_train,\n",
        "    \"data_val\": X_test,\n",
        "    \"labels_val\": y_test,\n",
        "    \"data_test\": X_val,\n",
        "    \"labels_test\": y_val,\n",
        "}\n",
        "\n",
        "print(\"Data shapes:\")\n",
        "for k, v in data.items():\n",
        "    print(\"Name:  {} \\t Shape: {}\".format(k, v.shape))\n",
        "print(\"\\nClass labels:\\n\", np.unique(y_train))\n",
        "\n",
        "# One image from each class\n",
        "def show_one_image_per_class(X, y, num_classes=10):\n",
        "    # Find one image for each class\n",
        "    images = []\n",
        "    labels = []\n",
        "\n",
        "    for cls in range(num_classes):\n",
        "        idx = np.where(y == cls)[0][0] # Get the first occurrence of each class\n",
        "        images.append(X[idx])\n",
        "        labels.append(y[idx])\n",
        "\n",
        "    # Define row and column structure\n",
        "    row_col_structure = [4, 4, 2]\n",
        "\n",
        "    fig, axes = plt.subplots(3, max(row_col_structure), figsize=(7, 7))\n",
        "    fig.suptitle(\"One Image per Class (0-9)\", fontsize=16)\n",
        "\n",
        "    img_idx = 0\n",
        "    for row, cols in enumerate(row_col_structure):\n",
        "        for col in range(cols):\n",
        "            ax = axes[row, col]\n",
        "            img = images[img_idx].reshape(28, 28)\n",
        "            ax.imshow(img, cmap='gray')\n",
        "            ax.axis('off')\n",
        "            ax.set_title(f\"Label: {labels[img_idx]}\")\n",
        "            img_idx += 1\n",
        "\n",
        "        # Hide unused subplots in each row\n",
        "        for col in range(cols, max(row_col_structure)):\n",
        "            axes[row, col].axis('off')\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.9])\n",
        "    plt.show()\n",
        "\n",
        "# Call the function\n",
        "show_one_image_per_class(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ibtpX7jMABm"
      },
      "outputs": [],
      "source": [
        "def rel_error(x, y, eps=1e-8):\n",
        "    \"\"\"Returns the relative error between x and y.\"\"\"\n",
        "    return np.max(np.abs(x - y) / np.maximum(eps, np.abs(x) + np.abs(y)))\n",
        "\n",
        "\n",
        "def eval_numerical_gradient_array(f, x, df, eps=1e-5):\n",
        "    \"\"\"\n",
        "    Computes the numerical gradient of a function that returns a scalar.\n",
        "\n",
        "    Arguments:\n",
        "        f: function that accepts an array x and returns a scalar output.\n",
        "        x: numpy array, the input.\n",
        "        df: upstream gradient.\n",
        "        eps: small step for finite differences.\n",
        "\n",
        "    Returns:\n",
        "        The numerical gradient (of f with respect to x) multiplied by df.\n",
        "    \"\"\"\n",
        "    grad = np.zeros_like(x)\n",
        "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
        "    while not it.finished:\n",
        "        idx = it.multi_index\n",
        "        old_val = x[idx]\n",
        "\n",
        "        x[idx] = old_val + eps\n",
        "        pos = f(x)\n",
        "        x[idx] = old_val - eps\n",
        "        neg = f(x)\n",
        "        x[idx] = old_val\n",
        "        grad[idx] = (np.sum(pos) - np.sum(neg)) / (2 * eps)\n",
        "\n",
        "        it.iternext()\n",
        "    return grad * df\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ws1C453UTxNM"
      },
      "source": [
        "## **Flatten Layer**   \n",
        "The `flatten` class is a neural network layer that reshapes the input from a multi-dimensional tensor to a **2D matrix**, where the first dimension represents the **batch size**, and the second dimension flattens all other dimensions.\n",
        "\n",
        "---\n",
        "\n",
        "## **Implementation Details**  \n",
        "\n",
        "### **`forward(x)`**   (2 points)\n",
        "- Takes an input `x` of shape **(N, d1, d2, ..., dK)** and reshapes it into **(N, -1)**.  \n",
        "- Stores the original shape (`x`) in `self.meta` to restore it during the backward pass.  \n",
        "- Returns the reshaped output.  \n",
        "\n",
        "---\n",
        "\n",
        "### **`backward(dout)`**  (2 points)\n",
        "- Restores the original shape of `dout` using `self.meta`, ensuring correct gradient flow.  \n",
        "- Returns `dx`, which has the same shape as the original input.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hAddn_h3QKD6"
      },
      "outputs": [],
      "source": [
        "class flatten(object):\n",
        "    \"\"\"\n",
        "    Reshapes the input from (N, d1, d2, ..., dK) to (N, -1).\n",
        "    \"\"\"\n",
        "    def __init__(self, name=\"flatten\"):\n",
        "        self.name = name\n",
        "        self.params = {}\n",
        "        self.grads = {}\n",
        "        self.meta = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        # ====== TODO ======\n",
        "        pass\n",
        "        # ========================================\n",
        "        self.meta = x\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        x = self.meta\n",
        "        # ====== TODO ======\n",
        "        pass\n",
        "        # ========================================\n",
        "        self.meta = None\n",
        "        return dx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RiND4GvYQK7t"
      },
      "outputs": [],
      "source": [
        "print(\"========== Test flatten layer ==========\")\n",
        "\n",
        "input_bz = 2\n",
        "input_dim = (3,4)\n",
        "x = np.random.randn(input_bz, *input_dim)\n",
        "flatten_test_layer = flatten(name=\"flatten_test\")\n",
        "\n",
        "out = flatten_test_layer.forward(x)\n",
        "print(\"Flatten forward shape check:\", out.shape, \"should be (2, 12).\")\n",
        "\n",
        "dout = np.random.randn(*out.shape)\n",
        "dx = flatten_test_layer.backward(dout)\n",
        "print(\"Flatten backward shape check:\", dx.shape, \"should be (2, 3, 4).\")\n",
        "\n",
        "def forward_func(xx):\n",
        "    out_ = flatten_test_layer.forward(xx)\n",
        "    return np.sum(out_ * dout)\n",
        "\n",
        "dx_num = eval_numerical_gradient_array(forward_func, x, 1.0)\n",
        "print(\"Flatten dx error:\", rel_error(dx, dx_num), \"should be around 1e-10.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YnkC_eufOdp2"
      },
      "source": [
        "## **Fully-Connected Layer**\n",
        "The `fc class` is a neural network layer that performs an affine transformation: y = xW + b, where W is the weight matrix and b is the bias vector.\n",
        "\n",
        "---\n",
        "\n",
        "## **Implementation Details**\n",
        "### **`forward(x)`** (2 points)\n",
        "- Takes an input x of shape (N, D) and computes y = xW + b.\n",
        "- Stores the input (x) in self.meta for use during the backward pass.\n",
        "Returns the computed output.\n",
        "\n",
        "### **`backward(dout)`**  (2 points)\n",
        "\n",
        "- Retrieves the cached input (x) from self.meta.\n",
        "- Computes gradients with respect to the input (dx), weights (dw), and biases (db).\n",
        "- Saves dw and db in self.grads.\n",
        "- Returns dx, ensuring proper gradient flow."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AkGNpWrY4TFq"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TGVEbe0_sFhQ"
      },
      "outputs": [],
      "source": [
        "class fc(object):\n",
        "    \"\"\"\n",
        "    Fully-connected layer: y = xW + b.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim, output_dim, init_scale=1e-2, name=\"fc\"):\n",
        "        self.name = name\n",
        "        # Name the weight and bias for easy lookup in dictionaries.\n",
        "        self.w_name = name + \"_w\"\n",
        "        self.b_name = name + \"_b\"\n",
        "\n",
        "        # Initialize parameters\n",
        "        self.params = {}\n",
        "        self.grads = {}\n",
        "\n",
        "        self.params[self.w_name] = init_scale * np.random.randn(input_dim, output_dim)\n",
        "        self.params[self.b_name] = np.zeros(output_dim)\n",
        "\n",
        "        self.grads[self.w_name] = None\n",
        "        self.grads[self.b_name] = None\n",
        "\n",
        "        self.meta = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        # ====== TODO ======\n",
        "        pass\n",
        "        # ========================================\n",
        "        self.meta = x\n",
        "        return out\n",
        "\n",
        "\n",
        "    def backward(self, dout):\n",
        "        x = self.meta\n",
        "        # ====== TODO ======\n",
        "        pass\n",
        "        # ========================================\n",
        "        self.grads[self.w_name] = dw\n",
        "        self.grads[self.b_name] = db\n",
        "        self.meta = None\n",
        "        return dx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kzAS7pBBsCaS"
      },
      "outputs": [],
      "source": [
        "print(\"========== Test fc layer forward ==========\")\n",
        "fc_test_layer = fc(12, 5, init_scale=1e-2, name=\"fc_test\")\n",
        "fc_test_layer.params[\"fc_test_w\"] = np.linspace(-0.2, 0.3, num=12*5).reshape(12,5)\n",
        "fc_test_layer.params[\"fc_test_b\"] = np.linspace(-0.1, 0.1, num=5)\n",
        "\n",
        "test_out = fc_test_layer.forward(out)\n",
        "print(\"FC forward output:\", test_out)\n",
        "print(\"Check shape:\", test_out.shape, \"should be (2,5).\")\n",
        "\n",
        "print(\"========== Test fc layer backward ==========\")\n",
        "dout2 = np.random.randn(*test_out.shape)\n",
        "dx2 = fc_test_layer.backward(dout2)\n",
        "print(\"dx2 shape:\", dx2.shape, \"should be (2,12) to match flatten out's shape.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vlcf64X94xsy"
      },
      "source": [
        "## **GELU Activation Function (`gelu`)**  \n",
        "The **Gaussian Error Linear Unit (GELU)** is a smooth and differentiable activation function often used in transformer-based architectures like BERT. It is an alternative to ReLU and Swish.\n",
        "\n",
        "## **Implementation Details**  \n",
        "\n",
        "### **`forward(x)`**  (2 points)\n",
        "- Computes the GELU activation using the **approximate formula**:\n",
        "  \\[\n",
        "  y = 0.5 * x * (1 + tanh( sqrt(2/pi) * (x + 0.044715*x^3) ))\n",
        "  \\]\n",
        "- Stores `x` in `self.meta` to use in the backward pass.\n",
        "- Returns the activated output.\n",
        "\n",
        "### **`backward(dout)`**  (2 points)\n",
        "- Computes the\n",
        "    \\[\n",
        "    y =0.5*(1 + tanh_) + 0.5*x*(1 - tanh_**2)*sqrt(2.0 / pi)*(1 + 3*0.044715*x**2)\n",
        "    \\]\n",
        "\n",
        "- Returns the activated output.\n",
        "\n",
        "---\n",
        "\n",
        "## **Dropout Layer (`dropout`)**  \n",
        "The **Dropout layer** is a regularization technique that randomly drops a fraction of neurons during training to prevent overfitting.\n",
        "\n",
        "## **Implementation Details**  \n",
        "\n",
        "### **`forward(x, is_training=True)`**  (2 points)\n",
        "- If `is_training=True`:\n",
        "  - Creates a **mask** with probability `keep_prob` to retain neurons.\n",
        "  - Applies the mask to the input `x` by multiplying the retained neurons.\n",
        "- If `is_training=False`, **dropout is disabled**, and the input is passed unchanged.\n",
        "- Stores `x` in `self.meta` for backpropagation.\n",
        "\n",
        "### **`backward(dout)`**  (2 points)\n",
        "- If dropout was applied, **scales the gradient using the same mask**.\n",
        "- Returns `dx`, the gradient with respect to the input.\n",
        "\n",
        "---\n",
        "\n",
        "## **Cross-Entropy Loss (`cross_entropy`)**  \n",
        "The **Cross-Entropy Loss** is used for **multi-class classification**. It measures how well the predicted probability distribution matches the true labels.\n",
        "\n",
        "## **Implementation Details**  \n",
        "\n",
        "### **`forward(logits, labels)`**  (2 points)\n",
        "- Applies **softmax** to transform logits into class probabilities.\n",
        "- Computes the negative log-likelihood loss:\n",
        "  \\[\n",
        "  L = - \\sum \\log P(\\text{correct class})\n",
        "  \\]\n",
        "- If `size_average=True`, **averages the loss over the batch**.\n",
        "- Stores `logits` and `labels` for the backward pass.\n",
        "- Returns the **scalar loss value**.\n",
        "\n",
        "### **`backward()`**  (2 points)\n",
        "- Computes the **gradient of the loss with respect to logits**:\n",
        "  \\[\n",
        "  \\frac{\\partial L}{\\partial x_i} = P(x_i) - 1 \\text{ (for the correct class)}\n",
        "  \\]\n",
        "- If `size_average=True`, the gradient is **normalized by batch size**.\n",
        "- Returns `dx`, the gradient of the loss.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U3PStPLoGp6Z"
      },
      "outputs": [],
      "source": [
        "def softmax(x):\n",
        "    \"\"\"\n",
        "    Computes the softmax function for the last dimension of x.\n",
        "\n",
        "    Arguments:\n",
        "        x: numpy array of shape (N, C)\n",
        "\n",
        "    Returns:\n",
        "        An array of the same shape as x with softmax applied.\n",
        "    \"\"\"\n",
        "    s = np.sum(np.exp(x), axis=-1, keepdims=True)\n",
        "    scores = np.exp(x) / s\n",
        "    return scores\n",
        "\n",
        "class gelu(object):\n",
        "    \"\"\"\n",
        "    retrieved from https://pytorch.org/docs/stable/generated/torch.nn.GELU.html\n",
        "    GeLU activation function (approximate version):\n",
        "    y = 0.5 * x * (1 + tanh( sqrt(2/pi) * (x + 0.044715*x^3) ))\n",
        "    \"\"\"\n",
        "    def __init__(self, name=\"gelu\"):\n",
        "        self.name = name\n",
        "        self.params = {}\n",
        "        self.grads = {}\n",
        "        self.meta = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        # ====== TODO ======\n",
        "        pass\n",
        "        # ========================================\n",
        "        self.meta = x\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        x = self.meta\n",
        "        # ====== TODO ======\n",
        "        pass\n",
        "        # ========================================\n",
        "        self.meta = None\n",
        "        return dx\n",
        "\n",
        "\n",
        "class dropout(object):\n",
        "    \"\"\"\n",
        "    Dropout layer.\n",
        "\n",
        "    Arguments:\n",
        "        keep_prob: probability of keeping a neuron.\n",
        "        seed: random seed for reproducibility.\n",
        "    \"\"\"\n",
        "    def __init__(self, keep_prob=1.0, seed=None, name=\"dropout\"):\n",
        "        self.name = name\n",
        "        self.params = {}\n",
        "        self.grads = {}\n",
        "        self.keep_prob = keep_prob\n",
        "        self.seed = seed\n",
        "        self.meta = None\n",
        "        self.mask = None\n",
        "        self.is_training = False\n",
        "        if seed is not None:\n",
        "            self.rng = np.random.RandomState(seed)\n",
        "        else:\n",
        "            self.rng = np.random\n",
        "\n",
        "    def forward(self, x, is_training=True):\n",
        "        self.is_training = is_training\n",
        "        if (self.keep_prob == 1.0) or (not is_training):\n",
        "            out = x\n",
        "            self.mask = np.ones_like(x)\n",
        "        else:\n",
        "            # ====== TODO ======\n",
        "            pass\n",
        "            # ========================================\n",
        "        self.meta = x\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        if (self.keep_prob == 1.0) or (not self.is_training):\n",
        "            dx = dout\n",
        "        else:\n",
        "            # ====== TODO ======\n",
        "            pass\n",
        "            # ========================================\n",
        "        self.meta = None\n",
        "        return dx\n",
        "\n",
        "\n",
        "class cross_entropy(object):\n",
        "    \"\"\"\n",
        "    Cross-entropy loss layer.\n",
        "    The forward pass returns a scalar loss, and the backward pass returns the gradient with respect to logits.\n",
        "    \"\"\"\n",
        "    def __init__(self, size_average=True):\n",
        "        self.size_average = size_average\n",
        "        self.logits = None\n",
        "        self.labels = None\n",
        "\n",
        "    def forward(self, logits, labels):\n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "            logits: numpy array of shape (N, C).\n",
        "            labels: numpy array of shape (N,) with integer labels in [0, C-1].\n",
        "\n",
        "        Returns:\n",
        "            Scalar loss value.\n",
        "        \"\"\"\n",
        "        probs = softmax(logits)\n",
        "        # ====== TODO ======\n",
        "        pass\n",
        "        # ========================================\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def backward(self):\n",
        "        \"\"\"\n",
        "        Returns:\n",
        "            Gradient with respect to logits, of shape (N, C).\n",
        "        \"\"\"\n",
        "        probs = self.logits\n",
        "        labels = self.labels\n",
        "        # ====== TODO ======\n",
        "        pass\n",
        "        # ========================================\n",
        "        return dx\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SodpKKok5fO0"
      },
      "outputs": [],
      "source": [
        "\n",
        "def forward_x(xx):\n",
        "    old_x = out.copy()\n",
        "    fc_test_layer.meta = xx\n",
        "    ret = fc_test_layer.forward(xx)\n",
        "    return ret\n",
        "\n",
        "some_input_x = out\n",
        "\n",
        "def forward_fc(xx):\n",
        "    out_ = fc_test_layer.forward(xx)\n",
        "    return np.sum(out_ * dout2)\n",
        "\n",
        "dx_num2 = eval_numerical_gradient_array(forward_fc, some_input_x, 1.0)\n",
        "print(\"fc backward dx error:\", rel_error(dx2, dx_num2), 'should be around 1e-10.')\n",
        "\n",
        "print(\"========== Test gelu forward/backward ==========\")\n",
        "gelu_layer = gelu(name=\"gelu_test\")\n",
        "x3 = np.random.randn(3,4)\n",
        "out3 = gelu_layer.forward(x3)\n",
        "dout3 = np.random.randn(*out3.shape)\n",
        "dx3 = gelu_layer.backward(dout3)\n",
        "\n",
        "def forward_gelu(xx):\n",
        "    out_ = gelu_layer.forward(xx)\n",
        "    return np.sum(out_ * dout3)\n",
        "\n",
        "dx_num3 = eval_numerical_gradient_array(forward_gelu, x3, 1.0)\n",
        "print(\"gelu backward dx error:\", rel_error(dx3, dx_num3), 'should be around 1e-10.')\n",
        "\n",
        "print(\"========== Test dropout forward/backward ==========\")\n",
        "drop_layer = dropout(keep_prob=0.5, seed=2023)\n",
        "x4 = np.random.randn(4,4)\n",
        "out4 = drop_layer.forward(x4, is_training=True)\n",
        "print(\"Dropout out shape:\", out4.shape)\n",
        "dout4 = np.random.randn(*out4.shape)\n",
        "dx4 = drop_layer.backward(dout4)\n",
        "\n",
        "print(\"========== Test cross_entropy ==========\")\n",
        "ce_layer = cross_entropy()\n",
        "test_logits = np.array([\n",
        "    [1.0, 2.0, 3.0],\n",
        "    [1.5, 0.9, -1.0]\n",
        "])\n",
        "test_labels = np.array([2, 0])\n",
        "\n",
        "loss = ce_layer.forward(test_logits, test_labels)\n",
        "dx_ = ce_layer.backward()\n",
        "print(\"cross_entropy loss:\", loss, 'should be around 0.448')\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AGBP7mi56OL3"
      },
      "outputs": [],
      "source": [
        "class sequential(object):\n",
        "    \"\"\"\n",
        "    A sequential container.\n",
        "    Modules will be added to it in the order they are passed in the constructor.\n",
        "    \"\"\"\n",
        "    def __init__(self, *layers):\n",
        "        self.layers = []\n",
        "        self.params = {}\n",
        "        self.grads = {}\n",
        "        for layer in layers:\n",
        "            self.layers.append(layer)\n",
        "            for pname, pval in layer.params.items():\n",
        "                self.params[pname] = pval\n",
        "            for gname, gval in layer.grads.items():\n",
        "                self.grads[gname] = gval\n",
        "\n",
        "    def forward(self, x, is_training=True):\n",
        "        out = x\n",
        "        for layer in self.layers:\n",
        "            if layer.name.startswith(\"dropout\"):\n",
        "                out = layer.forward(out, is_training=is_training)\n",
        "            else:\n",
        "                out = layer.forward(out)\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        for layer in reversed(self.layers):\n",
        "            dout = layer.backward(dout)\n",
        "            for gname, grad_val in layer.grads.items():\n",
        "                self.grads[gname] = grad_val\n",
        "        return dout\n",
        "\n",
        "    def assign(self, name, value):\n",
        "        self.params[name] = value\n",
        "        for layer in self.layers:\n",
        "            if name in layer.params:\n",
        "                layer.params[name] = value\n",
        "\n",
        "    def assign_grads(self, name, value):\n",
        "        self.grads[name] = value\n",
        "        for layer in self.layers:\n",
        "            if name in layer.grads:\n",
        "                layer.grads[name] = value\n",
        "\n",
        "\n",
        "class FashionMnistNet(object):\n",
        "    \"\"\"\n",
        "    test your implementation with this network\n",
        "    \"\"\"\n",
        "    def __init__(self, hidden_dim=100, keep_prob=1.0, init_scale=1e-2):\n",
        "        self.net = sequential(\n",
        "            flatten(name=\"flat\"),\n",
        "            fc(28*28, hidden_dim, init_scale=init_scale, name=\"fc1\"),\n",
        "            gelu(name=\"gelu1\"),\n",
        "            dropout(keep_prob=keep_prob, name=\"dropout1\"),\n",
        "            fc(hidden_dim, 10, init_scale=init_scale, name=\"fc2\"),\n",
        "        )\n",
        "        self.params = self.net.params\n",
        "        self.grads  = self.net.grads\n",
        "\n",
        "    def forward(self, x, is_training=True):\n",
        "        return self.net.forward(x, is_training=is_training)\n",
        "\n",
        "    def backward(self, dout):\n",
        "        return self.net.backward(dout)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rET4RQr8Llco"
      },
      "outputs": [],
      "source": [
        "def accuracy(logits, labels):\n",
        "    pred = np.argmax(logits, axis=-1)\n",
        "    return np.mean(pred == labels)\n",
        "\n",
        "def train_network(net,\n",
        "                  X_tr, y_tr,\n",
        "                  X_val, y_val,\n",
        "                  lr=1e-2,\n",
        "                  batch_size=128,\n",
        "                  epochs=5):\n",
        "    \"\"\"\n",
        "    Train the network.\n",
        "    \"\"\"\n",
        "    loss_func = cross_entropy(size_average=True)\n",
        "\n",
        "    N = X_tr.shape[0]\n",
        "    train_acc_list = []\n",
        "    val_acc_list = []\n",
        "    loss_list = []\n",
        "    for ep in range(epochs):\n",
        "        # shuffle\n",
        "        idx = np.random.permutation(N)\n",
        "        X_tr_ = X_tr[idx]\n",
        "        y_tr_ = y_tr[idx]\n",
        "\n",
        "        num_batch = int(np.ceil(N / batch_size))\n",
        "        for b in range(num_batch):\n",
        "            start = b*batch_size\n",
        "            end = min((b+1)*batch_size, N)\n",
        "            xb = X_tr_[start:end]\n",
        "            yb = y_tr_[start:end]\n",
        "\n",
        "            # forward\n",
        "            logits = net.forward(xb, is_training=True)\n",
        "            if np.isnan(logits).any():\n",
        "                print(\"Forward got NaN in logits\")\n",
        "            loss_val = loss_func.forward(logits, yb)\n",
        "\n",
        "            # backward\n",
        "            dLogits = loss_func.backward()\n",
        "            net.backward(dLogits)\n",
        "            for pname in net.params:\n",
        "                if net.grads[pname] is None:\n",
        "                    print(\"Gradient is None for param:\", pname)\n",
        "            for pname in net.params:\n",
        "                net.params[pname] -= lr * net.grads[pname]\n",
        "\n",
        "        # train/val acc\n",
        "        loss_list.append(loss_val)\n",
        "        train_preds = net.forward(X_tr, is_training=False)\n",
        "        val_preds   = net.forward(X_val, is_training=False)\n",
        "        train_acc = accuracy(train_preds, y_tr)\n",
        "        val_acc = accuracy(val_preds, y_val)\n",
        "        train_acc_list.append(train_acc)\n",
        "        val_acc_list.append(val_acc)\n",
        "        print(f\"Epoch {ep+1}/{epochs}: loss={loss_val:.4f}, train_acc={train_acc:.4f}, val_acc={val_acc:.4f}\")\n",
        "\n",
        "    return loss_list, train_acc_list, val_acc_list\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GwGvc7YYZbIn"
      },
      "outputs": [],
      "source": [
        "# Training parameters\n",
        "hidden_dim = 32\n",
        "keep_prob=0.8\n",
        "init_scale=1e-2\n",
        "epochs = 20\n",
        "lr = 1e-3\n",
        "batch_size =32\n",
        "\n",
        "# Training Loop\n",
        "model = FashionMnistNet(hidden_dim=hidden_dim, keep_prob=keep_prob, init_scale=init_scale)\n",
        "loss_list, train_acc_list, val_acc_list = train_network(\n",
        "    model, X_train, y_train, X_val, y_val,\n",
        "    lr=lr, batch_size=batch_size, epochs=epochs\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bC8-1BS_Z1w8"
      },
      "outputs": [],
      "source": [
        "# Test\n",
        "test_preds = model.forward(X_test, is_training=False)\n",
        "test_acc = accuracy(test_preds, y_test)\n",
        "print(\"Final test accuracy:\", test_acc)\n",
        "\n",
        "# Plot 1: Train Loss\n",
        "plt.figure(figsize=(7, 3))\n",
        "plt.title(\"Train Loss\")\n",
        "plt.plot(loss_list, '-o', color='tab:red', label=\"Train Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# Plot 2: Train and Validation Accuracy\n",
        "plt.figure(figsize=(7, 4))\n",
        "plt.title(\"Training and Validation Accuracy\")\n",
        "plt.plot(train_acc_list, '-o', label=\"Train Accuracy\")\n",
        "plt.plot(val_acc_list, '-o', label=\"Validation Accuracy\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWRLYBxQdKYF"
      },
      "source": [
        "# Implementing different optimizers (2 points per optimizazer, 10 points total)\n",
        "In this part of the assignment, you will be asked to implement a few different commonly used optimizers inside of neural network. After the implementation, we will create a simple neural network to see how much of different in model convergence can be impacted by optimizer alone.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Ir0tiHueL7F"
      },
      "source": [
        "## SGD (Stochastic Gradient Descent):\n",
        "SGD is a fundamental optimization algorithm that updates model parameters by computing the gradient of the loss with respect to a single example or a mini-batch. Its simplicity and computational efficiency make it a baseline method for training models, though it can be sensitive to noise and may converge slowly in certain scenarios."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9onW09Kpx1a3"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class SGD:\n",
        "    def __init__(self, learning_rate):\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "    def update(self, params, grads):\n",
        "        # params: list of numpy arrays (the current weights)\n",
        "        # grads: list of numpy arrays (the corresponding gradients)\n",
        "        updated_params = []\n",
        "        for p, g in zip(params, grads):\n",
        "            # ====== TODO ======\n",
        "            pass\n",
        "            # updated_param =\n",
        "            # ========================================\n",
        "            updated_params.append(updated_param)\n",
        "        return updated_params\n",
        "\n",
        "N = 4\n",
        "D = 5\n",
        "p = np.linspace(-0.7, 0.4, num=N*D).reshape(N, D)\n",
        "grad = 2 * p  # For each element, gradient = 2*x.\n",
        "learning_rate = 0.1\n",
        "epsilon = 1e-8\n",
        "\n",
        "# ------------------------\n",
        "# Test SGD:\n",
        "expected_sgd = [[-0.56      , -0.51368421, -0.46736842, -0.42105263, -0.37473684],\n",
        " [-0.32842105, -0.28210526, -0.23578947, -0.18947368, -0.14315789],\n",
        " [-0.09684211, -0.05052632, -0.00421053,  0.04210526,  0.08842105],\n",
        " [ 0.13473684,  0.18105263,  0.22736842,  0.27368421,  0.32      ]]\n",
        "sgd_optimizer = SGD(learning_rate=learning_rate)\n",
        "updated_sgd = sgd_optimizer.update([p.copy()], [grad])[0]\n",
        "if np.allclose(updated_sgd, expected_sgd, atol=1e-6):\n",
        "    print(\"SGD test passed!\")\n",
        "else:\n",
        "    print(\"SGD test failed: expected\", expected_sgd, \"got\", updated_sgd)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zk3oUDy6x2qL"
      },
      "source": [
        "## SGD with Momentum:\n",
        "This variant of SGD introduces a momentum term that accumulates past gradients to smooth updates. By incorporating a velocity vector, the optimizer is better able to overcome small local minima and oscillations, often resulting in faster convergence. The momentum term helps the algorithm \"remember\" previous directions, thereby accelerating progress in relevant directions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8OgfA60yx2R-"
      },
      "outputs": [],
      "source": [
        "class SGDMomentum:\n",
        "    def __init__(self, learning_rate, momentum=0.9):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.momentum = momentum\n",
        "        self.velocity = None  # This will be a list of velocities for each parameter\n",
        "\n",
        "    def update(self, params, grads):\n",
        "        if self.velocity is None:\n",
        "            self.velocity = [np.zeros_like(p) for p in params]\n",
        "        updated_params = []\n",
        "        for i, (p, g) in enumerate(zip(params, grads)):\n",
        "            # ====== TODO ======\n",
        "            pass\n",
        "            # updated_param =\n",
        "            # ========================================\n",
        "            updated_params.append(updated_param)\n",
        "        return updated_params\n",
        "\n",
        "momentum = 0.9\n",
        "sgdm_optimizer = SGDMomentum(learning_rate=learning_rate, momentum=momentum)\n",
        "updated_mom_first = sgdm_optimizer.update([p.copy()], [grad])[0]\n",
        "if np.allclose(updated_mom_first, expected_sgd, atol=1e-6):\n",
        "    print(\"SGDMomentum first update test passed!\")\n",
        "else:\n",
        "    print(\"SGDMomentum first update test failed: expected\", expected_sgd, \"got\", updated_mom_first)\n",
        "\n",
        "expected_sgdm_second = [[-0.322     , -0.29536842, -0.26873684, -0.24210526, -0.21547368],\n",
        " [-0.18884211, -0.16221053, -0.13557895, -0.10894737, -0.08231579],\n",
        " [-0.05568421, -0.02905263, -0.00242105,  0.02421053,  0.05084211],\n",
        " [ 0.07747368,  0.10410526,  0.13073684,  0.15736842,  0.184     ],]\n",
        "\n",
        "updated_mom_second = sgdm_optimizer.update([updated_mom_first.copy()], [2 * updated_mom_first])[0]\n",
        "if np.allclose(updated_mom_second, expected_sgdm_second, atol=1e-6):\n",
        "    print(\"SGDMomentum second update test passed!\")\n",
        "else:\n",
        "    print(\"SGDMomentum second update test failed: expected\", expected_sgdm_second, \"got\", updated_mom_second)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65ow0Av6x68F"
      },
      "source": [
        "## RMSProp:\n",
        "RMSProp (Root Mean Square Propagation) is an adaptive learning rate method that divides the learning rate for each parameter by a running average of the magnitudes of recent gradients. This normalization helps to balance the step sizes and stabilize the training process, particularly in non-stationary or noisy environments. RMSProp effectively addresses the diminishing learning rate issue seen in earlier methods like AdaGrad."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Lzwu_Bhx9lc"
      },
      "outputs": [],
      "source": [
        "class RMSProp:\n",
        "    def __init__(self, learning_rate, decay_rate=0.99, epsilon=1e-8):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.decay_rate = decay_rate\n",
        "        self.epsilon = epsilon\n",
        "        self.cache = None  # to store running average of squared gradients\n",
        "\n",
        "    def update(self, params, grads):\n",
        "        if self.cache is None:\n",
        "            self.cache = [np.zeros_like(p) for p in params]\n",
        "        updated_params = []\n",
        "        for i, (p, g) in enumerate(zip(params, grads)):\n",
        "            # ====== TODO ======\n",
        "            pass\n",
        "            # updated_param =\n",
        "            # ========================================\n",
        "            updated_params.append(updated_param)\n",
        "        return updated_params\n",
        "\n",
        "\n",
        "expected_rmsprop = [[ 0.3        , 0.35789474,  0.41578947,  0.47368421,  0.53157895],\n",
        " [ 0.58947368,  0.64736842,  0.70526316,  0.76315789,  0.82105263],\n",
        " [ 0.87894737,  0.93684211,  0.99473684, -0.94736842, -0.88947368],\n",
        " [-0.83157895, -0.77368421, -0.71578947, -0.65789474, -0.6       ],]\n",
        "rmsprop_optimizer = RMSProp(learning_rate=learning_rate, decay_rate=0.99, epsilon=epsilon)\n",
        "updated_rmsprop = rmsprop_optimizer.update([p.copy()], [grad])[0]\n",
        "if np.allclose(updated_rmsprop, expected_rmsprop, atol=1e-6):\n",
        "    print(\"RMSProp test passed!\")\n",
        "else:\n",
        "    print(\"RMSProp test failed: expected\", expected_rmsprop, \"got\", updated_rmsprop)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NbJNHWalyAr6"
      },
      "source": [
        "## Adam (Adaptive Moment Estimation):\n",
        "Adam combines the benefits of both momentum (by maintaining an exponentially decaying average of past gradients) and adaptive learning rates (by keeping a moving average of squared gradients). Additionally, it applies bias correction to these estimates, which improves convergence, especially in problems with large parameter spaces and noisy gradients. Adam is widely used because it typically requires little tuning and performs robustly across various tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dLhSX_-CyDg-"
      },
      "outputs": [],
      "source": [
        "class Adam:\n",
        "    def __init__(self, learning_rate, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.beta1 = beta1  # decay rate for first moment\n",
        "        self.beta2 = beta2  # decay rate for second moment\n",
        "        self.epsilon = epsilon\n",
        "        self.m = None  # first moment\n",
        "        self.v = None  # second moment\n",
        "        self.t = 0     # time step\n",
        "\n",
        "    def update(self, params, grads):\n",
        "        if self.m is None:\n",
        "            self.m = [np.zeros_like(p) for p in params]\n",
        "            self.v = [np.zeros_like(p) for p in params]\n",
        "        self.t += 1\n",
        "        updated_params = []\n",
        "        for i, (p, g) in enumerate(zip(params, grads)):\n",
        "            # ====== TODO ======\n",
        "            # Update biased first moment estimate\n",
        "\n",
        "            # Update biased second raw moment estimate\n",
        "\n",
        "            # Compute bias-corrected first moment estimate\n",
        "\n",
        "            # Compute bias-corrected second moment estimate\n",
        "\n",
        "            # Update parameter\n",
        "            # updated_param =\n",
        "            pass\n",
        "            # ========================================\n",
        "            updated_params.append(updated_param)\n",
        "        return updated_params\n",
        "\n",
        "expected_adam = [[-0.6       , -0.54210526, -0.48421053, -0.42631579, -0.36842105],\n",
        " [-0.31052632, -0.25263158, -0.19473684, -0.13684211, -0.07894737],\n",
        " [-0.02105263,  0.03684211,  0.09473684, -0.04736842,  0.01052632],\n",
        " [ 0.06842105,  0.12631579,  0.18421053,  0.24210526,  0.3       ],]\n",
        "adam_optimizer = Adam(learning_rate=learning_rate, beta1=0.9, beta2=0.999, epsilon=epsilon)\n",
        "updated_adam = adam_optimizer.update([p.copy()], [grad])[0]\n",
        "if np.allclose(updated_adam, expected_adam, atol=1e-6):\n",
        "    print(\"Adam test passed!\")\n",
        "else:\n",
        "    print(\"Adam test failed: expected\", expected_adam, \"got\", updated_adam)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJJuv0sPyFM9"
      },
      "source": [
        "## AdamW (Adam with Decoupled Weight Decay):\n",
        "AdamW is a modification of the Adam optimizer that decouples the weight decay (regularization) term from the gradient-based update. In standard Adam, weight decay is often implemented in a way that can interfere with the adaptive learning rate mechanism. By decoupling these two aspects, AdamW allows for more effective regularization, often leading to improved generalization in deep learning models.\n",
        "\n",
        "Hint: AdamW optimizer is an extended version of Adam optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VykbFXfjyGrc"
      },
      "outputs": [],
      "source": [
        "class AdamW:\n",
        "    def __init__(self, learning_rate, beta1=0.9, beta2=0.999, epsilon=1e-8, weight_decay=0.01):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.beta1 = beta1\n",
        "        self.beta2 = beta2\n",
        "        self.epsilon = epsilon\n",
        "        self.weight_decay = weight_decay\n",
        "        self.m = None\n",
        "        self.v = None\n",
        "        self.t = 0\n",
        "\n",
        "    def update(self, params, grads):\n",
        "        if self.m is None:\n",
        "            self.m = [np.zeros_like(p) for p in params]\n",
        "            self.v = [np.zeros_like(p) for p in params]\n",
        "        self.t += 1\n",
        "        updated_params = []\n",
        "        for i, (p, g) in enumerate(zip(params, grads)):\n",
        "            # ====== TODO ======\n",
        "            pass\n",
        "            # updated_param =\n",
        "            # ========================================\n",
        "            updated_params.append(updated_param)\n",
        "        return updated_params\n",
        "\n",
        "weight_decay = 0.01\n",
        "expected_adamw = [[-0.5993    , -0.54146316, -0.48362632, -0.42578947, -0.36795263],\n",
        " [-0.31011579, -0.25227895, -0.19444211, -0.13660526, -0.07876842],\n",
        " [-0.02093158,  0.03690526,  0.09474211, -0.04742105,  0.01041579],\n",
        " [ 0.06825263,  0.12608947,  0.18392632,  0.24176316,  0.2996    ]]\n",
        "adamw_optimizer = AdamW(learning_rate=learning_rate, beta1=0.9, beta2=0.999,\n",
        "                        epsilon=epsilon, weight_decay=weight_decay)\n",
        "updated_adamw = adamw_optimizer.update([p.copy()], [grad])[0]\n",
        "if np.allclose(updated_adamw, expected_adamw, atol=1e-6):\n",
        "    print(\"AdamW test passed!\")\n",
        "else:\n",
        "    print(\"AdamW test failed: expected\", expected_adamw, \"got\", updated_adamw)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zUlvZ80ayIiu"
      },
      "source": [
        "## Compare the performance of different optimizers with a simple neural network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LhhmJy6XyOy1"
      },
      "outputs": [],
      "source": [
        "class SimpleNN:\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        # Initialize weights and biases\n",
        "        self.params = {\n",
        "            'W1': np.random.randn(input_dim, hidden_dim) * 0.01,\n",
        "            'b1': np.zeros((1, hidden_dim)),\n",
        "            'W2': np.random.randn(hidden_dim, output_dim) * 0.01,\n",
        "            'b2': np.zeros((1, output_dim))\n",
        "        }\n",
        "\n",
        "    def forward(self, X):\n",
        "        # Hidden layer\n",
        "        self.cache = {}\n",
        "        self.cache['Z1'] = np.dot(X, self.params['W1']) + self.params['b1']\n",
        "        self.cache['A1'] = np.tanh(self.cache['Z1'])\n",
        "        # Output layer (sigmoid for binary classification)\n",
        "        self.cache['Z2'] = np.dot(self.cache['A1'], self.params['W2']) + self.params['b2']\n",
        "        self.cache['A2'] = 1 / (1 + np.exp(-self.cache['Z2']))\n",
        "        return self.cache['A2']\n",
        "\n",
        "    def compute_loss(self, A2, y):\n",
        "        m = y.shape[0]\n",
        "        # Binary cross-entropy loss\n",
        "        loss = -1/m * np.sum(y * np.log(A2 + 1e-8) + (1 - y) * np.log(1 - A2 + 1e-8))\n",
        "        return loss\n",
        "\n",
        "    def backward(self, X, y):\n",
        "        m = y.shape[0]\n",
        "        grads = {}\n",
        "        A2 = self.cache['A2']\n",
        "        dZ2 = A2 - y.reshape(-1, 1)  # assuming y is a vector of 0s and 1s\n",
        "        grads['W2'] = np.dot(self.cache['A1'].T, dZ2) / m\n",
        "        grads['b2'] = np.sum(dZ2, axis=0, keepdims=True) / m\n",
        "        dA1 = np.dot(dZ2, self.params['W2'].T)\n",
        "        dZ1 = dA1 * (1 - self.cache['A1'] ** 2)  # derivative of tanh\n",
        "        grads['W1'] = np.dot(X.T, dZ1) / m\n",
        "        grads['b1'] = np.sum(dZ1, axis=0, keepdims=True) / m\n",
        "        return grads"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wEbfKLGMyfVs"
      },
      "outputs": [],
      "source": [
        "def SimpleNN_train(network, optimizer, optimizer_label, X_train, y_train, X_val, y_val, epochs=1000, print_every=100):\n",
        "    loss_history = []\n",
        "    train_acc_history = []\n",
        "    val_acc_history = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # --- Forward pass on training data ---\n",
        "        A2_train = network.forward(X_train)\n",
        "        loss = network.compute_loss(A2_train, y_train)\n",
        "        loss_history.append(loss)\n",
        "\n",
        "        # Compute training accuracy (threshold at 0.5)\n",
        "        train_preds = (A2_train > 0.5).astype(int)\n",
        "        train_acc = np.mean(train_preds == y_train)\n",
        "        train_acc_history.append(train_acc)\n",
        "\n",
        "        # --- Backward pass and parameter update ---\n",
        "        grads = network.backward(X_train, y_train)\n",
        "        # Gather parameters and corresponding gradients as lists\n",
        "        params = [network.params['W1'], network.params['b1'],\n",
        "                  network.params['W2'], network.params['b2']]\n",
        "        grads_list = [grads['W1'], grads['b1'], grads['W2'], grads['b2']]\n",
        "        updated_params = optimizer.update(params, grads_list)\n",
        "        network.params['W1'], network.params['b1'], network.params['W2'], network.params['b2'] = updated_params\n",
        "\n",
        "        # --- Forward pass on validation data ---\n",
        "        A2_val = network.forward(X_val)\n",
        "        val_preds = (A2_val > 0.5).astype(int)\n",
        "        val_acc = np.mean(val_preds == y_val)\n",
        "        val_acc_history.append(val_acc)\n",
        "\n",
        "        if epoch % print_every == 0:\n",
        "            print(f\"{optimizer_label} Epoch {epoch:04d}: Loss = {loss:.4f}, Train Acc = {train_acc*100:.2f}%, Val Acc = {val_acc*100:.2f}%\")\n",
        "\n",
        "    return loss_history, train_acc_history, val_acc_history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4NFut_8KyRam"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generate a synthetic binary classification dataset\n",
        "def generate_data(n_samples=500):\n",
        "    np.random.seed(42)\n",
        "    X = np.random.randn(n_samples, 2)\n",
        "    # Label is 1 if the sum of squares > median, else 0\n",
        "    y = (np.sum(X**2, axis=1) > np.median(np.sum(X**2, axis=1))).astype(int)\n",
        "    return X, y.reshape(-1,1)\n",
        "\n",
        "X, y = generate_data(500)\n",
        "split_idx = int(0.8 * X.shape[0])\n",
        "X_train, y_train = X[:split_idx], y[:split_idx]\n",
        "X_val, y_val = X[split_idx:], y[split_idx:]\n",
        "\n",
        "# Initialize networks (all networks share the same architecture)\n",
        "net_sgd = SimpleNN(input_dim=2, hidden_dim=10, output_dim=1)\n",
        "net_mom = SimpleNN(input_dim=2, hidden_dim=10, output_dim=1)\n",
        "net_rms = SimpleNN(input_dim=2, hidden_dim=10, output_dim=1)\n",
        "net_adam = SimpleNN(input_dim=2, hidden_dim=10, output_dim=1)\n",
        "net_adamw = SimpleNN(input_dim=2, hidden_dim=10, output_dim=1)\n",
        "\n",
        "# Create optimizer instances with chosen hyperparameters\n",
        "optimizer_sgd = SGD(learning_rate=0.1)\n",
        "optimizer_mom = SGDMomentum(learning_rate=0.1, momentum=0.9)\n",
        "optimizer_rms = RMSProp(learning_rate=0.01, decay_rate=0.99, epsilon=1e-8)\n",
        "optimizer_adam = Adam(learning_rate=0.01, beta1=0.9, beta2=0.999, epsilon=1e-8)\n",
        "optimizer_adamw = AdamW(learning_rate=0.01, beta1=0.9, beta2=0.999, epsilon=1e-8, weight_decay=0.01)\n",
        "\n",
        "# Train networks and record loss history\n",
        "loss_sgd, train_acc_sgd, val_acc_sgd   = SimpleNN_train(net_sgd, optimizer_sgd, 'sgd', X, y, X_val, y_val, epochs=1000, print_every=200)\n",
        "loss_mom, train_acc_mom, val_acc_mom   = SimpleNN_train(net_mom, optimizer_mom, 'sgdm', X, y, X_val, y_val, epochs=1000, print_every=200)\n",
        "loss_rms, train_acc_rms, val_acc_rms   = SimpleNN_train(net_rms, optimizer_rms, 'rms', X, y, X_val, y_val, epochs=1000, print_every=200)\n",
        "loss_adam, train_acc_adam, val_acc_adam  = SimpleNN_train(net_adam, optimizer_adam, 'adam', X, y, X_val, y_val, epochs=1000, print_every=200)\n",
        "loss_adamw, train_acc_adamw, val_acc_adamw = SimpleNN_train(net_adamw, optimizer_adamw, 'adamW', X, y, X_val, y_val, epochs=1000, print_every=200)\n",
        "\n",
        "# Plot training loss curves for each optimizer\n",
        "plt.figure(figsize=(8,10))\n",
        "plt.subplot(3, 1, 1)\n",
        "plt.plot(loss_sgd, label='SGD')\n",
        "plt.plot(loss_mom, label='SGD with Momentum')\n",
        "plt.plot(loss_rms, label='RMSProp')\n",
        "plt.plot(loss_adam, label='Adam')\n",
        "plt.plot(loss_adamw, label='AdamW')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Optimizer Performance Comparison')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(3, 1, 2)\n",
        "plt.plot(train_acc_sgd, label='SGD')\n",
        "plt.plot(train_acc_mom, label='SGD with Momentum')\n",
        "plt.plot(train_acc_rms, label='RMSProp')\n",
        "plt.plot(train_acc_adam, label='Adam')\n",
        "plt.plot(train_acc_adamw, label='AdamW')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Training Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(3, 1, 3)\n",
        "plt.plot(val_acc_sgd, label='SGD')\n",
        "plt.plot(val_acc_mom, label='SGD with Momentum')\n",
        "plt.plot(val_acc_rms, label='RMSProp')\n",
        "plt.plot(val_acc_adam, label='Adam')\n",
        "plt.plot(val_acc_adamw, label='AdamW')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Validation Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01y9XbnOCDVX"
      },
      "source": [
        "# Using the plot above, answer the following questions:\n",
        "\n",
        "1. What did you observe from the plot? (2 points)\n",
        "2. Why does SGD gives a nearly flat loss curve, and how to fix it? (2 points)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJEg_BrgiOkF"
      },
      "source": [
        "## Plot the Activation Functions (2 points per activation function, 16 points total)\n",
        "In each of the activation function, use the given lambda function template to plot their corresponding curves. Do not use implemented library functions.\n",
        "\\begin{align*}\n",
        "1.\\ & \\textbf{Sigmoid Function:} \\quad \\sigma(x) = \\frac{1}{1 + e^{-x}} \\\\[10pt]\n",
        "2.\\ & \\textbf{Tanh (Hyperbolic Tangent):} \\quad f(x) = \\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}} \\\\[10pt]\n",
        "3.\\ & \\textbf{ReLU (Rectified Linear Unit):} \\quad f(x) = \\max(0, x) \\\\[10pt]\n",
        "4.\\ & \\textbf{Leaky ReLU (Rectified Linear Unit):} \\quad\n",
        "   f(x) =\n",
        "   \\begin{cases}\n",
        "      x, & \\text{if } x > 0 \\\\\n",
        "      \\alpha x, & \\text{if } x \\leq 0\n",
        "   \\end{cases} \\\\[10pt]\n",
        "5.\\ & \\textbf{ELU (Exponential Linear Unit):} \\quad\n",
        "   f(x) =\n",
        "   \\begin{cases}\n",
        "      x, & \\text{if } x > 0 \\\\\n",
        "      \\alpha (e^x - 1), & \\text{if } x \\leq 0\n",
        "   \\end{cases} \\\\[10pt]\n",
        "6.\\ & \\textbf{Softplus Function:} \\quad \\text{Softplus}(x) = \\ln(1 + e^x) \\\\[10pt]\n",
        "7.\\ & \\textbf{GELU (Gaussian Error Linear Unit):} \\quad\n",
        "   \\text{GELU}(x) = \\frac{1}{2}x \\left(1 + \\tanh \\left[\\sqrt{\\frac{2}{\\pi}} \\left(x + 0.044715x^3\\right)\\right]\\right) \\\\[10pt]\n",
        "8.\\ & \\textbf{Softmax Function:} \\quad\n",
        "   \\text{Softmax}(x_i) = \\frac{e^{x_i}}{\\sum_{j=1}^{n} e^{x_j}}\n",
        "\\end{align*}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F7Q5XyyMiXme"
      },
      "outputs": [],
      "source": [
        "left, right = -10, 10\n",
        "X  = np.linspace(left, right, 100)\n",
        "XS = np.linspace(-5, 5, 10)\n",
        "lw = 3\n",
        "alpha = 0.1 # alpha for leaky_relu and elu\n",
        "\n",
        "# ====== TODO ======\n",
        "sigmoid =\n",
        "tanh =\n",
        "relu =\n",
        "leaky_relu =\n",
        "elu =\n",
        "softplus =\n",
        "gelu =\n",
        "softmax =\n",
        "# ========================================\n",
        "\n",
        "activations = {\n",
        "    \"Sigmoid\": sigmoid,\n",
        "    \"Tanh\": tanh,\n",
        "    \"ReLU\": relu,\n",
        "    \"LeakyReLU\": leaky_relu,\n",
        "    \"ELU\": elu,\n",
        "    \"Softplus\": softplus,\n",
        "    \"Gelu\": gelu,\n",
        "    \"Softmax\": softmax,\n",
        "}\n",
        "\n",
        "\n",
        "# Ground Truth activations\n",
        "GT_Act = {\n",
        "    \"Sigmoid\": [0.00669285, 0.02005754, 0.0585369,  0.1588691,  0.36457644,\n",
        "                0.63542356, 0.8411309,  0.9414631,  0.97994246, 0.99330715],\n",
        "\n",
        "    \"Tanh\": [-0.9999092,  -0.99916247, -0.99229794, -0.93110961, -0.5046724,\n",
        "             0.5046724, 0.93110961,  0.99229794,  0.99916247,  0.9999092],\n",
        "\n",
        "    \"ReLU\": [0., 0., 0., 0., 0., 0.55555556, 1.66666667, 2.77777778, 3.88888889, 5.],\n",
        "\n",
        "    \"LeakyReLU\": [-0.05, -0.03888889, -0.02777778, -0.01666667, -0.00555556,\n",
        "                  0.55555556, 1.66666667, 2.77777778, 3.88888889, 5.],\n",
        "\n",
        "    \"ELU\": [-0.99326205, -0.97953192, -0.93782348, -0.8111244, -0.42624658,\n",
        "            0.55555556, 1.66666667, 2.77777778, 3.88888889, 5.],\n",
        "\n",
        "    \"Softplus\": [0.00671535, 0.02026142, 0.06032013, 0.17300799, 0.45346348,\n",
        "                 1.00901904, 1.83967466, 2.83809791, 3.90915031, 5.00671535],\n",
        "\n",
        "    \"Gelu\": [-2.29179620e-07, -1.18062300e-04, -7.13321406e-03, -7.98024027e-02,\n",
        "             -1.60723677e-01,  3.94831879e-01,  1.58686426e+00, 2.77064456e+00,\n",
        "             3.88877083e+00, 4.99999977e+00],\n",
        "\n",
        "    \"Softmax\": [3.04550464e-05, 9.25142622e-05, 2.81033514e-04, 8.53704437e-04,\n",
        "                2.59332510e-03, 7.87782605e-03, 2.39307225e-02, 7.26951163e-02,\n",
        "                2.20828265e-01, 6.70817038e-01]\n",
        "}\n",
        "\n",
        "fig = plt.figure(figsize=(10,10))\n",
        "for i, label in enumerate(activations):\n",
        "    ax = fig.add_subplot(3, 3, i+1)\n",
        "    ax.plot(X, activations[label](X), color=(52/255,120/255,198/255), lw=lw, label=label)\n",
        "    # print(activations[label](XS))\n",
        "    assert rel_error(activations[label](XS), GT_Act[label]) < 1e-6, \\\n",
        "           \"Your implementation of {} might be wrong\".format(label)\n",
        "    ax.axhline(0, color='black', lw = 0.5)\n",
        "    ax.axvline(0, color='black', lw = 0.5)\n",
        "    ax.set_title('{}'.format(label), fontsize=14)\n",
        "    fig.suptitle(\"Visualization of activation functions\", fontsize=16)\n",
        "\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
